quantizers:
  wrpn_quantizer:
    class: WRPNQuantizer
    bits_activations: 8
    bits_weights: 4
#    bits_overrides:
#    # Don't quantize first and last layer
#      conv1:
#        wts: null
#        acts: null


lr_schedulers:
  training_lr:
    class: MultiStepLR
    milestones: [50,75,100,150,175]
    gamma: 0.3

policies:
    - quantizer:
        instance_name: wrpn_quantizer
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1

    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1
